&emsp;2018年2月，我参加了某市智慧城市平台项目的研发，该项目的主要目的是经过多种渠道获取该市各种突发事件和各部门数据；通过对事件的紧急处理和数据的分析产生出决策并下发指令以提高办公效率。我在项目中承担架构设计师的职务，主要负责系统的架构设计以及部分软件设计工作。本文以智慧应急系统为例，主要讨论多层架构设计在该系统中的具体应用。整体划分了客户层、前端优化层、应用层、服务层、数据存储层、大数据存储层、大数据处理层的七层架构模式，最终项目顺利上线，并获得用户一致好评。
&emsp;随着互联网的崛起，某市响应政府的政策和号召，为了解决该城市中突发事件汇报不及时、上呈下达效率慢时间长、流程繁琐以及该市建设中不合理的隐匿问题以便更好的为人民服务；参考国外智能化城市建设的优势，决定发起智慧城市的项目。2018年2月，我司竞标该项目成功并承担了该市智慧城市项目中智慧应急系统的开发工作。该项目的主要目的是经过多种渠道获取该市各种事件信息（安保事故、消防事故、交通事故、地震、洪涝灾害等突发事件）和数据（主要是政府各职能部门如林业局，公安局，消防大队，具体涉及高铁、机场、林场、水库、公交、红绿灯、摄像头等相关设备的数据）；通过对事件的紧急处理和数据的分析产生出决策并下发指令以提高政府办公的速度，更快速更高效的解决突发事件。我在项目中担任架构设计师职务，架构小组共4人，我主要负责整体架构设计和部分软件设计，整个项目历时10个月，2018年12月顺利通过验收。
&emsp;由于智慧应急系统是智慧城市中最核心的项目，由于其业务的特殊性，对项目的可用性、性能、可修改性以及扩展性都要求很高，我选择了七层B/A/S作为该系统的软件体系结构，下面，我将分层次介绍七层架构中核心层次的设计过程。
&emsp;**1. 前端层**。智慧应急系统的界面大致包括公众号、人员管理系统、指挥调度中心、部委前置（不同部门）等多个部分的页面。使用前后端分离技术。前端采用Vue框架实现的H5页面实现。使用Vue的原因在于大部分前端开发都很熟练Vue，便于协作开发。其次是Vue框架目前已非常成熟，功能集成度高，可以很大程度上提升开发效率。采用H5的主要原因是一方面是由于其良好的跨平台性非常适合不同设备的适配，其次是不依赖浏览器插件可以实现画布、动画等一些高级功能。使用前后端分离的好处主要是为了与应用层解耦，不同模块的界面修改互不影响且可以由前端人员自行部署而不会影响到后端。JS和CSS都使用单独的域名多副本分布在不同机器上且使用了CDN技术存储最新的静态文件和图片让用户可以访问到最新的缓存，极大的提高了页面的渲染速度和用户体验。而前端采用DNS负载到多机房并配合反向代理进行转发，能很好的提升前端的性能和可用性。
&emsp;**2. 应用层**。后端整体采用微服务架构风格进行设计。应用层按照业务进行垂直拆分分为网格员应用、指挥调度中心应用、人员管理应用等多个应用部分。因为采用了前后端分离以及微服务架构，应用层采用Redis实现了session共享以及Spring的CorsFilter实现了前端层的跨域请求。另外每个应用都是以包装成微服务的形式实现，只是在职责上与服务层不同。这些应用主要是接收前端层的请求并通过RPC调用服务层处理后向前端层返回指定格式的数据。应用层存在的主要目的是与处理业务逻辑的服务层解耦；当前端层需要不同的格式的数据或改变数据内容，例如界面改版等，只需要修改应用层的数据渲染逻辑即可而无需改动服务层的业务逻辑，提升了系统的可修改性。
&emsp;**3. 服务层**。服务层是真正处理具体业务逻辑的部分。它不光为现有的应用层提供服务接口还为外部的职能部门的系统提供数据上传接口。服务层的质量直接影响整个系统的质量，是至关重要的一层。因此对服务层的性能以及可用性要求是很高的。除了将每个服务多副本分布部署外。我们还使用各种手段来保证服务层的质量，主要手段包括负载均衡、消息中间件、熔断器、限流机制、分布式缓存、链路追踪。对于负载均衡不必多说，服务间调用是采用的微服务组件自带的软负载机制，至于负载算法大部分配置的是基于响应时间优先的方式来合理分配流量。对于消息中间件，底层使用的是第三方的kafka，原因是kafka是真正的分布式中间件，它的一个Topic可分为多个partition，每个partition分为一个leader和多个follower副本机制实现读写分离并分布在多台机器上而且通过独特的参数配置可以保证数据的零丢失，在高性能、高可用、高扩展方面都有着优异的表现。我们通过消息中间件发布订阅以及推拉消息机制在服务层实现解耦、异步、削峰的作用，可以极大的提升服务层的性能、可修改性并且简化了服务设计。对于限流机制我们采用了guava包下的RateLimiter对服务层的高并发接口进行限流，它通过令牌桶算法能很好实现平滑突发限流和平滑预热限流，不会因并发过高而导致服务崩溃，很大程度的提升了系统的稳定性。另外由于服务众多，服务间调用频繁，一旦某个调用链中的某个服务出现故障会迟迟无法响应导致调用方以及上游服务IO线程无法释放也跟着陆续崩溃从而造成服务雪崩故障，为了避免这种事故的发生，我们在服务间调用时引入熔断器机制；熔断器会获取所有流量结果进行统计，当发现故障时立即处于打开状态让所有请求快速失败，后续进入半开状态会根据少数流量的统计结果决定断路器是打开还是关闭，可以很好的进行容错来避免服务雪崩。为了支撑指挥调度系统相关服务的高并发，实现高性能，我们引入分布式内存数据库Redis Cluster，RedisCluster采用16384个slot分布在多个主从集群上来可以实现实现海量数据存储和高可用，另外由于服务层数据粒度较大，我们采用在代码层面自行设计缓存结构以及失效时机，很大程度上保证了数据的一致性且提升了服务的性能。
&emsp;**4. 数据存储层**。在智慧应急系统中，最核心的模块是指挥调度相关的服务，这些服务都采用集中式存储。因为数据量和TPS都很高。我采用的措施有分库分表加主从读写分离的方式实现。底层使用shardingJdbc将MySQL数据库分为32个库，数据量大的表如事件表能分到500张表。shardingjdbc会根据配置的路由键自动将数据分散在对应库的对应表中，它还提供了主从读写分离的机制。很大程度上解决了海量数据的存储以及数据库的瓶颈问题。之所以开始就分很多库是为了方便扩展，开始32个库放在四台机器上，每天机器运行8个库，后续需要扩展时只要增加机器将其中的库移到上边修改下配置即可而不需要进行动态的rehash迁移。另外为了避免分布式环境下的主键冲突问题，我们采用雪花算法根据机房号、机器编号、时间戳来生成唯一的主键。在表的设计层面，我们采用索引和一些反规范化机制，例如增加冗余列、增加派生列、重新组表、垂直分表等来提升查询性能。在代码层面我们使用了Spring的cache注解将查询内容进行分布式缓存，从而减轻数据库的访问压力。
&emsp;2018年12月，项目正式交付，至今已稳定运行3年左右。尚未出现大的故障。当然在测试过程中我也发现了一些不足，当并发量小时雪花算法不出现冲突的情况下后面几位二进制都为0，导致数据大部分都存在编号为偶数的库，已经通过重写该算法进行了完善。我会继续努力提高自己的架构设计水平和技术能力，争取设计出更多高质量的项目。
